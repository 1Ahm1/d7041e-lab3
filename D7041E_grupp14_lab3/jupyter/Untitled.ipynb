{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84d145ba-4d37-41da-9e6c-60b626645590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data took 0.13060331344604492 seconds\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found array with 0 sample(s) (shape=(0, 100)) while a minimum of 1 is required.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 177\u001b[0m\n\u001b[0;32m    175\u001b[0m som\u001b[38;5;241m.\u001b[39mn_columns \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m21\u001b[39m\n\u001b[0;32m    176\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m--> 177\u001b[0m \u001b[43msom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    178\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining took \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend_time\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mstart_time\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\susi\\SOMClassifier.py:310\u001b[0m, in \u001b[0;36mSOMClassifier.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: Sequence, y: Optional[Sequence] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    288\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Fit classification SOM to the input data.\u001b[39;00m\n\u001b[0;32m    289\u001b[0m \n\u001b[0;32m    290\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    308\u001b[0m \n\u001b[0;32m    309\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 310\u001b[0m     X, y \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_estimation_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_classification\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    311\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX_: np\u001b[38;5;241m.\u001b[39mndarray \u001b[38;5;241m=\u001b[39m X\n\u001b[0;32m    312\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my_: np\u001b[38;5;241m.\u001b[39mndarray \u001b[38;5;241m=\u001b[39m y\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\susi\\SOMUtils.py:93\u001b[0m, in \u001b[0;36mcheck_estimation_input\u001b[1;34m(X, y, is_classification)\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Check input arrays.\u001b[39;00m\n\u001b[0;32m     72\u001b[0m \n\u001b[0;32m     73\u001b[0m \u001b[38;5;124;03mThis function is adapted from sklearn.utils.validation.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     90\u001b[0m \n\u001b[0;32m     91\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_classification:\n\u001b[1;32m---> 93\u001b[0m     X, y \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_X_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     95\u001b[0m     X, y \u001b[38;5;241m=\u001b[39m check_X_y(X, y, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat64)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\validation.py:1146\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1141\u001b[0m         estimator_name \u001b[38;5;241m=\u001b[39m _check_estimator_name(estimator)\n\u001b[0;32m   1142\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1143\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires y to be passed, but the target y is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1144\u001b[0m     )\n\u001b[1;32m-> 1146\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1148\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1149\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1150\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1151\u001b[0m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1152\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1153\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1154\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_2d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1155\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1156\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1157\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1158\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1159\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1160\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1162\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric, estimator\u001b[38;5;241m=\u001b[39mestimator)\n\u001b[0;32m   1164\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\validation.py:967\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    965\u001b[0m     n_samples \u001b[38;5;241m=\u001b[39m _num_samples(array)\n\u001b[0;32m    966\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_samples \u001b[38;5;241m<\u001b[39m ensure_min_samples:\n\u001b[1;32m--> 967\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    968\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m sample(s) (shape=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m) while a\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    969\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m minimum of \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m is required\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    970\u001b[0m             \u001b[38;5;241m%\u001b[39m (n_samples, array\u001b[38;5;241m.\u001b[39mshape, ensure_min_samples, context)\n\u001b[0;32m    971\u001b[0m         )\n\u001b[0;32m    973\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_min_features \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m array\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m    974\u001b[0m     n_features \u001b[38;5;241m=\u001b[39m array\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[1;31mValueError\u001b[0m: Found array with 0 sample(s) (shape=(0, 100)) while a minimum of 1 is required."
     ]
    }
   ],
   "source": [
    "import re\n",
    "import time\n",
    "import string\n",
    "# pip install nltk\n",
    "import nltk\n",
    "from nltk import ngrams\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "# pip install matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "# pip install susi\n",
    "import susi\n",
    "# pip install -U scikit-learn\n",
    "import sklearn.metrics\n",
    "import pandas as pd\n",
    "# pip install seaborn\n",
    "import seaborn as sn\n",
    "\n",
    "\n",
    "\n",
    "d = 100\n",
    "H = {}\n",
    "s = {}\n",
    "s_test = {}\n",
    "n = 3\n",
    "np.random.seed(42)\n",
    "language_codes = ['bg', 'cs',  'da', 'de', 'el', 'en', 'es', 'et', 'fi', 'fr', 'hu', 'it', 'lt', 'lv', 'nl', 'pl', 'pt', 'ro', 'sk', 'sl', 'sv']\n",
    "permutation = np.random.permutation(d)\n",
    "training_path = 'data/training/'\n",
    "test_path = 'data/test/'\n",
    "X_train = np.empty((400000, d))\n",
    "X_test = np.empty((400000, d))\n",
    "Y_train = np.empty((400000, ), dtype=object)\n",
    "Y_test = np.empty((400000, ), dtype=object)\n",
    "H_n_gram = {}\n",
    "# Function to generate n-grams from a sentence\n",
    "def generate_ngrams(text):\n",
    "    # Generate n-grams\n",
    "    n_grams = list(ngrams(list(text), n))\n",
    "    \n",
    "    return n_grams\n",
    "    \n",
    "def bind(H1, H2):\n",
    "    return np.multiply(H1, H2)\n",
    "\n",
    "def permute(H, r):\n",
    "    for _ in range(r):\n",
    "        H = H[permutation]\n",
    "    return H\n",
    "\n",
    "def bundle(H1, H2):\n",
    "    return H1 + H2\n",
    "    \n",
    "def generate_hd_vector_letter():\n",
    "    return np.random.choice([-1, 1], size = d)\n",
    "\n",
    "def generate_hd_vector_n_gram(n_gram):\n",
    "    if n_gram in H_n_gram:\n",
    "        return H_n_gram[n_gram]\n",
    "        \n",
    "    result = np.ones(d)\n",
    "    for i in range(n):\n",
    "        result = bind(permute(H[n_gram[i]], i + 1), result)\n",
    "    H_n_gram[n_gram] = result\n",
    "    return result\n",
    "    \n",
    "def generate_hd_vector_text(n_grams, scope):  \n",
    "    n_grams = set(n_grams)\n",
    "    result = np.zeros(d)\n",
    "    for n_gram in n_grams:\n",
    "        rep = s[n_gram] if scope == 'train' else s_test[n_gram]\n",
    "        result += rep * generate_hd_vector_n_gram(n_gram)\n",
    "    \n",
    "    return np.interp(result, (result.min(), result.max()), (-1, 1))\n",
    "\n",
    "def preprocess_text(sentence):\n",
    "    # Remove punctuation using a regular expression\n",
    "    expr = r'[^\\w\\s]'\n",
    "    #expr = r'[^a-zA-Z\\s]'\n",
    "    #expr = r'[^a-zA-Z\\sÀ-ÖØ-öø-ÿ]'\n",
    "    sentence = re.sub(expr, '', sentence)\n",
    "    # Convert to lowercase\n",
    "    sentence = sentence.lower()\n",
    "    return sentence\n",
    "    \n",
    "#def preprocess_text(sentence):\n",
    "#    # Remove punctuation\n",
    "#    sentence = sentence.translate(str.maketrans('', '', string.punctuation))\n",
    "#    # Convert to lowercase\n",
    "#    sentence = sentence.lower()\n",
    "#    return sentence\n",
    "ind_train = 0\n",
    "ind_test = 0\n",
    "def load_training_data(file_path, ind):\n",
    "    global X_train, Y_train, ind_train\n",
    "    i = 0\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            \n",
    "            # Split the line into number and sentence\n",
    "            parts = line.strip().split('\\t', 1)\n",
    "            if len(parts) == 2:\n",
    "                _, text = parts\n",
    "                text = preprocess_text(text)\n",
    "                for letter in text:\n",
    "                    if not letter in H:\n",
    "                        H[letter] = generate_hd_vector_letter()  \n",
    "                n_grams = generate_ngrams(text)\n",
    "                for n_gram in n_grams:\n",
    "                    if n_gram in s:\n",
    "                        s[n_gram] += 1\n",
    "                    else:\n",
    "                        s[n_gram] = 1    \n",
    "                #X_train = np.vstack([X_train, generate_hd_vector_text(n_grams, 'train')])\n",
    "                #Y_train = np.append(Y_train, language_codes[ind])\n",
    "                X_train[ind_train] = generate_hd_vector_text(n_grams, 'train')\n",
    "                Y_train[ind_train] = language_codes[ind]    \n",
    "                ind_train += 1\n",
    "                if ind_train == 100000:\n",
    "                    break\n",
    "                        \n",
    "def load_test_data(file_path, ind):\n",
    "    global X_test, Y_test, ind_test\n",
    "    i = 0\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for text in file:\n",
    "            if text[0] != '<':\n",
    "                text = preprocess_text(text)\n",
    "                for letter in text:\n",
    "                    if not letter in H:\n",
    "                        H[letter] = generate_hd_vector_letter()  \n",
    "                n_grams = generate_ngrams(text)\n",
    "                for n_gram in n_grams:\n",
    "                    if n_gram in s_test:\n",
    "                        s_test[n_gram] += 1\n",
    "                    else:\n",
    "                        s_test[n_gram] = 1    \n",
    "                #X_test = np.vstack([X_test, generate_hd_vector_text(n_grams, 'test')])\n",
    "                #Y_test = np.append(Y_test, language_codes[ind])\n",
    "                X_test[ind_test] = generate_hd_vector_text(n_grams, 'test')\n",
    "                Y_test[ind_test] = language_codes[ind]\n",
    "                ind_test += 1\n",
    "                if ind_test == 100000:\n",
    "                    break\n",
    "                \n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for i in range(len(language_codes)):\n",
    "    load_training_data(f\"{training_path}/{language_codes[i]}.txt\", i)\n",
    "    load_test_data(f\"{test_path}/{language_codes[i]}.txt\", i)\n",
    "    print(f\"loading data for {language_codes[i]} done ({i+1}/{len(language_codes)})\")\n",
    "    \n",
    "train_permutation = np.random.permutation(ind_train)\n",
    "test_permutation = np.random.permutation(ind_test)\n",
    "\n",
    "X_train = X_train[0:ind_train][train_permutation]\n",
    "Y_train = Y_train[0:ind_train][train_permutation]\n",
    "X_test = X_test[0:ind_test][test_permutation]\n",
    "Y_test = Y_test[0:ind_test][test_permutation]\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"loading data took {end_time - start_time} seconds\")\n",
    "\n",
    "# initialize and fit SuSi\n",
    "som = susi.SOMClassifier()\n",
    "# som.n_iter_supervised = 1000\n",
    "# som.train_mode_supervised = \"mean\"\n",
    "som.n_rows = 1\n",
    "som.n_columns = 21\n",
    "start_time = time.time()\n",
    "som.fit(X_train, Y_train)\n",
    "end_time = time.time()\n",
    "print(f\"training took {end_time - start_time} seconds\")\n",
    "\n",
    "# predict and calculate the accuracy score\n",
    "y_pred = som.predict(X_test)\n",
    "print(som.score(X_test, Y_test))\n",
    "\n",
    "def pred(X_test, som):\n",
    "    mini = 1\n",
    "    res = 20\n",
    "    \n",
    "    yy = np.empty((0, 1))\n",
    "    for x in X_test:\n",
    "        i = 0\n",
    "        for ww in som.unsuper_som_:\n",
    "            for w in ww:\n",
    "                coss = np.dot(x, w)/(np.linalg.norm(x)*np.linalg.norm(w))\n",
    "                if coss < mini:\n",
    "                    mini = coss\n",
    "                    res = i\n",
    "                i += 1\n",
    "        yy = np.append(yy, res)\n",
    "    return yy\n",
    "    \n",
    "# y = pred(X_test, som)\n",
    "# dif = abs(Y_test-y)\n",
    "# print(len([x for x in dif if int(x)==0])/len(dif))\n",
    "\n",
    "conf_mat= sklearn.metrics.confusion_matrix(Y_test, y_pred, labels=None, sample_weight=None)\n",
    "\n",
    "#plot confusion matrix\n",
    "%matplotlib inline\n",
    "df_cm = pd.DataFrame(conf_mat, index = [i for i in language_codes], columns = [j for j in language_codes])\n",
    "plt.figure(figsize = (18,18))\n",
    "sn.heatmap(df_cm, annot=True)\n",
    "        \n",
    "                \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "60538381-4141-4473-8833-76b724003599",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['da' 'nl' 'en' ... 'nl' 'fr' 'lt']\n"
     ]
    }
   ],
   "source": [
    "np.save('X_train.npy', X_train)\n",
    "np.save('Y_train.npy', Y_train)\n",
    "np.save('X_test.npy', X_test)\n",
    "np.save('Y_test.npy', Y_test)\n",
    "# Load training and test data\n",
    "X_train = np.load('X_train.npy', allow_pickle = True)\n",
    "Y_train = np.load('Y_train.npy', allow_pickle = True)\n",
    "X_test = np.load('X_test.npy', allow_pickle = True)\n",
    "Y_test = np.load('Y_test.npy', allow_pickle = True)\n",
    "print(Y_train)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "48ece3ae-a1ee-4741-afbb-aedc8b2e5956",
   "metadata": {},
   "source": [
    "print(Y_train[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
